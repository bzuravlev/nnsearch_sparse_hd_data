{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0013312-3fbe-48f2-8d32-77b8f4e6c965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def compute_vgg16_descriptors(image_folder):\n",
    "    model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    descriptors = np.zeros((0,512))\n",
    "\n",
    "    for filename in sorted(os.listdir(image_folder)):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(image_folder, filename)\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            img_data = image.img_to_array(img)\n",
    "            img_data = np.expand_dims(img_data, axis=0)\n",
    "            img_data = preprocess_input(img_data)\n",
    "            \n",
    "            features = model.predict(img_data)\n",
    "            r = features.flatten()\n",
    "            \n",
    "            descriptors = np.vstack([descriptors, r])\n",
    "\n",
    "    return descriptors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa34026-9866-4302-93c4-684e6e86ec5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf17584-20bd-46ab-bd9e-43cc96b7a2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./data/nordland/raw/summer', descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a265920-40c2-4dc8-b78f-ad03c8ca15a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "691d8780-0f40-4aac-bff0-0a2b698d861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def compute_custom_vgg16_descriptors(image_folder):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(4096, activation='relu'),  # Adding a dense layer with 4096 units\n",
    "        Dense(4096, activation='relu')   # Another dense layer to increase dimensionality\n",
    "    ])\n",
    "    \n",
    "    descriptors = np.zeros((0,4096))\n",
    "\n",
    "    for filename in sorted(os.listdir(image_folder)):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(image_folder, filename)\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            img_data = image.img_to_array(img)\n",
    "            img_data = np.expand_dims(img_data, axis=0)\n",
    "            img_data = preprocess_input(img_data)\n",
    "            \n",
    "            features = model.predict(img_data)\n",
    "            r = features.flatten()\n",
    "            descriptors = np.vstack([descriptors, r])\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "\n",
    "# Output is suppressed, and descriptors are stored in the `descriptors` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ad1fcf5-91dc-4e53-b124-88a258b3c951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./data/nordland/raw/summer', descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c24f140-bf47-46bf-b300-45c58f3711a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def compute_high_dimensional_descriptors(image_folder, output_dim=4096):\n",
    "    # Load the VGG16 model without the top layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    \n",
    "    # Create a custom model with an additional Dense layer with tanh activation\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(output_dim, activation='tanh')  # 4096 dimensions with tanh activation\n",
    "    ])\n",
    "    \n",
    "    descriptors = np.zeros((0,4096))\n",
    "\n",
    "    for filename in sorted(os.listdir(image_folder)):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(image_folder, filename)\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            img_data = image.img_to_array(img)\n",
    "            img_data = np.expand_dims(img_data, axis=0)\n",
    "            img_data = preprocess_input(img_data)\n",
    "            \n",
    "            # Compute the descriptor\n",
    "            features = model.predict(img_data)\n",
    "            r = features.flatten()\n",
    "            descriptors = np.vstack([descriptors, r])\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "## Specify the directory containing images\n",
    "#image_folder = '../images/nordland/summer'\n",
    "\n",
    "## Compute the high-dimensional descriptors\n",
    "#descriptors = compute_high_dimensional_descriptors(image_folder)\n",
    "\n",
    "#np.save('./data/nordland/raw/summer', descriptors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "16dc2d6a-3958-4ae3-8042-b59cbf91a05f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 4096)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5edc1e88-2ece-4981-a996-50a79e26533f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./data/nordland/raw/summer', descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23abe1-8d45-4ed4-851f-408a7a614061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pickle\n",
    "\n",
    "# Load the DELF model from TensorFlow Hub\n",
    "delf = hub.load('https://tfhub.dev/google/delf/1').signatures['default']\n",
    "\n",
    "def compute_delf_descriptors(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = tf.image.resize(img_rgb, (224, 224))  # Resize the image\n",
    "    img_resized = tf.image.convert_image_dtype(img_resized, tf.float32)\n",
    "    \n",
    "    # Compute DELF features\n",
    "    result = delf(image=tf.expand_dims(img_resized, axis=0))\n",
    "    \n",
    "    # Extract the descriptors\n",
    "    descriptors = result['descriptors'].numpy()\n",
    "    keypoints = result['keypoints'].numpy()\n",
    "    \n",
    "    return keypoints, descriptors\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    descriptors_list = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            keypoints, descriptors = compute_delf_descriptors(image_path)\n",
    "            descriptors_list.append((filename, keypoints, descriptors))\n",
    "    \n",
    "    return descriptors_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e92da-9dc1-4262-83bd-5ba9b8622722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your images folder\n",
    "folder_path = '/path/to/your/images'\n",
    "\n",
    "# Process the images\n",
    "descriptors_list = process_images_in_folder(folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b331ead-a665-44d2-bf7c-9c4a862023e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save descriptors and keypoints to a file\n",
    "with open('delf_descriptors.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptors_list, f)\n",
    "\n",
    "# Load the descriptors later if needed\n",
    "with open('delf_descriptors.pkl', 'rb') as f:\n",
    "    loaded_descriptors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7910f-e3c7-4dd0-96dc-77c3d7e4b58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e00f18d-2e03-4a12-927f-a23aa1ba50d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NetVLAD(nn.Module):\n",
    "    def __init__(self, num_clusters=64, dim=512):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.dim = dim\n",
    "        self.conv = nn.Conv2d(dim, num_clusters, kernel_size=(1, 1), bias=True)\n",
    "        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape  # N is batch size, C is the number of channels, H and W are height and width\n",
    "        soft_assign = self.conv(x)  # [N, num_clusters, H, W]\n",
    "        soft_assign = soft_assign.view(N, self.num_clusters, -1)  # [N, num_clusters, H*W]\n",
    "        soft_assign = F.softmax(soft_assign, dim=1)  # Apply softmax along the clusters\n",
    "        \n",
    "        x_flatten = x.view(N, C, -1)  # Flatten the feature maps to [N, C, H*W]\n",
    "        x_flatten = x_flatten.permute(0, 2, 1)  # Change to [N, H*W, C] for easier broadcasting\n",
    "        \n",
    "        vlad = torch.zeros([N, self.num_clusters, C], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "        for Ck in range(self.num_clusters):\n",
    "            residual = x_flatten - self.centroids[Ck:Ck + 1, :].expand(x_flatten.size(0), -1, -1)  # [N, H*W, C]\n",
    "            residual *= soft_assign[:, Ck:Ck + 1, :].permute(0, 2, 1)  # [N, H*W, 1]\n",
    "            vlad[:, Ck:Ck + 1, :] = residual.sum(dim=1)  # Sum over H*W dimension\n",
    "            \n",
    "        vlad = F.normalize(vlad, p=2, dim=2)  # intra-normalization\n",
    "        vlad = vlad.view(N, -1)  # Flatten to [N, num_clusters*C]\n",
    "        vlad = F.normalize(vlad, p=2, dim=1)  # L2 normalize\n",
    "        \n",
    "        return vlad\n",
    "\n",
    "def load_netvlad_model():\n",
    "    backbone = models.vgg16(pretrained=True)\n",
    "    netvlad_layer = nn.Sequential(\n",
    "        *list(backbone.features.children())[:-2],\n",
    "        NetVLAD(num_clusters=64)\n",
    "    )\n",
    "    return netvlad_layer\n",
    "\n",
    "def compute_netvlad_descriptor(image_path, model):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_rgb = cv2.resize(img_rgb, (224, 224))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(img_rgb).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        descriptor = model(img_tensor)\n",
    "    \n",
    "    return descriptor.cpu().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a976118b-36b1-49ec-b143-18d6c41e3829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "des = compute_netvlad_descriptor('../images/nordland/spring/nl_spring00001.png', load_netvlad_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "174a021d-1fea-4e5d-a10a-6e64cdc35f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 2759)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.rand(25, 3000)\n",
    "transformer = GaussianRandomProjection(random_state=rng)\n",
    "X_new = transformer.fit_transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32d4ef9a-237d-4093-ae49-90d4dfb15ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32768,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6feb28c4-c7fa-4c5e-aebd-024529d38c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_images_in_folder(folder_path, model):\n",
    "    descriptors_list = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            descriptor = compute_netvlad_descriptor(image_path, model)\n",
    "            descriptors_list.append(descriptor[0])\n",
    "    \n",
    "    return np.array(descriptors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "156064f1-049f-4e81-91ae-214b3a84e591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def preprocess_descriptors(desc_matr):\n",
    "    #gaussian random projection\n",
    "    transformer = GaussianRandomProjection()\n",
    "    desc_matr_new = transformer.fit_transform(desc_matr)\n",
    "    #l2 normalization\n",
    "    desc_matr_new = normalize(desc_matr_new, axis=1, norm='l2')\n",
    "    #dimensionwise standardization to standard normal distributions\n",
    "        # Step 1: Compute the mean and standard deviation for each column\n",
    "    mean = np.mean(desc_matr_new, axis=0)  # Mean along columns\n",
    "    std_dev = np.std(desc_matr_new, axis=0)  # Standard deviation along columns\n",
    "\n",
    "        # Step 2: Standardize the columns\n",
    "    desc_matr_standardized = (desc_matr_new - mean) / std_dev\n",
    "    return desc_matr_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9485750a-e95b-4bd9-82c6-20babbdf2922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bz/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/bz/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration creation descriptors: 41.70160608399601 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "folder_path = '../images/nordland/spring'\n",
    "model = load_netvlad_model()\n",
    "start = perf_counter()\n",
    "descriptors_list = process_images_in_folder(folder_path, model)\n",
    "end = perf_counter()\n",
    "duration = end - start\n",
    "print(f\"Duration creation descriptors: {duration} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9c63bb7-02a9-4392-9648-60261d1abdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration creation descriptors: 3.373913374991389 seconds\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "descriptors_list_pp = preprocess_descriptors(descriptors_list)\n",
    "end = perf_counter()\n",
    "duration = end - start\n",
    "print(f\"Duration creation descriptors: {duration} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffaf3242-02a9-4e54-adc2-5abbf6e7567a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 5483)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors_list_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b999f34-6c88-4e7e-b6bb-b7e166f5246a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./data/nordland/raw/spring', descriptors_list_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "461dc99d-8088-400f-bd65-5259d639f482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bz/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/bz/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration creation descriptors: 39.1135114159988 seconds\n",
      "Duration creation descriptors: 3.3795906659943284 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "folder_path = '../images/nordland/winter'\n",
    "model = load_netvlad_model()\n",
    "start = perf_counter()\n",
    "descriptors_list = process_images_in_folder(folder_path, model)\n",
    "end = perf_counter()\n",
    "duration = end - start\n",
    "print(f\"Duration creation descriptors: {duration} seconds\")\n",
    "start = perf_counter()\n",
    "descriptors_list_pp = preprocess_descriptors(descriptors_list)\n",
    "end = perf_counter()\n",
    "duration = end - start\n",
    "print(f\"Duration creation descriptors: {duration} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6aa3324-46b8-475a-856c-82e779656b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./data/nordland/raw/winter', descriptors_list_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aefb262a-09d6-4bfc-a11d-0b398cabeeff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 5483)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors_list_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa7a18-d338-456e-bdbd-030e46b9501c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
